matrix(1:6, 2, 3) %*% matrix(3:8, 3, 2)
1:3 %*% 1:3
matrix(1:9,3,3) %*% 1:3
`matrix(1:4, 2, 2)
)
matrix(1:4, 2, 2)
t(matrix(1:4, 2, 2))
diag(1, 3)
skip()
skip()
skip()
skip()
main
main()
swirl()
x <- list(iris=iris, cars=cars, n=2)
x$n
mode(x[[3]])
mode(x[3])
x[["n"]]
x&n
x$n
a <- data.frame(class = "NTU", id = 1:10,scores = matrix(c(80:99),nrow=10,ncol=2 ) )
a[,]
a[,1]
a[1,2]
colnames(a)
a[2,"id"]
a[1:2,1:2]
class(a[1:2,1:12])
class(a[1:2,1:2])
`a[1:2, "id", drop= FALSE]
''
a[1:2, "id", drop= FALSE]
#' 請問CO2 有多少列(row)？
answer1 <- nrow(CO2) #請將NULL替換成你的程式碼
#' 請問CO2 有多少欄(column)？
answer2 <- ncol(CO2) #請將NULL替換成你的程式碼
#' 請問CO2 的各欄的名稱為何？
answer3 <- colnames(CO2) #請將NULL替換成你的程式碼
#' 請問uptake這欄的平均值為多少？
answer4 <- mean(CO2$uptake) #請將NULL替換成你的程式碼
#' CO2 共有很多很多列（answer1），
#' 請從CO2 中挑出一些列，滿足以下的條件：
#' 這些列的uptake值，超過全部CO2"平均"的uptake值
#' （`answer4`）
#'
#'   你可以先取出uptake的向量、接著拿該向量和平均值做比較、把結果的logical vector丟到`[]`的第一個參數中。
answer5 <- CO2$uptake[CO2$uptake>answer4] #請將NULL替換成你的程式碼
answer5
submit()
answer5 <- CO2[CO2$uptake>answer4] #請將NULL替換成你的程式碼
#' CO2 共有很多很多列（answer1），
#' 請從CO2 中挑出一些列，滿足以下的條件：
#' 這些列的uptake值，超過全部CO2"平均"的uptake值
#' （`answer4`）
#'
#'   你可以先取出uptake的向量、接著拿該向量和平均值做比較、把結果的logical vector丟到`[]`的第一個參數中。
answer5 <- CO2[CO2$uptake>answer4] #請將NULL替換成你的程式碼
CO2$uptake>answer4
#' CO2 共有很多很多列（answer1），
#' 請從CO2 中挑出一些列，滿足以下的條件：
#' 這些列的uptake值，超過全部CO2"平均"的uptake值
#' （`answer4`）
#'
#'   你可以先取出uptake的向量、接著拿該向量和平均值做比較、把結果的logical vector丟到`[]`的第一個參數中。
answer5 <- CO2[CO2$uptake>answer4,] #請將NULL替換成你的程式碼
answer5
#' 請問Type有多少種類別？
#' ps. answer6 應該是一個整數
answer6 <- class(CO2$Type) #請將NULL替換成你的程式碼
main(()
main()
swirl()
datasets
library(datasets)
library(help=datasets)
iris <- 1
data(iris,package = "datasets")
head(iris)
colnames(iris)
help(iris)
readBin(lvr_land.path, what = "raw", n = 3)
eadLines(file(lvr_land.path, encoding = "BIG5"),n = 1)
readLines(file(lvr_land.path, encoding = "BIG5"),n = 1)
readLines(file(lvr_land.path, encoding = "BIG5"),n = 5)
file.info(lvr_land.path)
lvr_land.info <- file.info(lvr_land.path)
class(lvr_land.info)
colnames(lvr_land.info)
lvr_land.info$size
lvr_land.bin <- readBin(lvr_land.path,what = "raw", n = 3)
lvr_land.bin <- readBin(lvr_land.path,what = "raw", n = lvr_land.info$size)
library(stringi)
lvr_land.txt <- stri_encode(lvr_land.bin, "BIG-5",
"UTF-8")
read.table(lvr_land.path, fileEncoding = "BIG-5")
lvr_land <- read.table(lvr_land.path,fileEncoding = "BIG5")
lvr_land <- read.table(file(lvr_land.path, encoding = "BIG5"), header =
TRUE, sep = ",")
main()
library(help="datasets")
library(help="datasets")
library(help=datasets)
iris
iris<-1
data(iris,package = "datasets")
head(iris)
colnames(iris)
help(iris)
readBin(lvr_land.path,
what = "raw", n = 3)
data = iris
head(data)
apply(data,2,mean)
apply(data,mean)
head(data)
data[,5]
head(data)
data[,-5]
data = data[,-5]
head(data)
apply(data,mean)
apply(data,2,mean)
apply(data,1,mean)
?apply
IrisData <- iris
iris[,5]
library(swirl)
swirl()
1:18
x <- matrix(1:18,6,3)
?matrix
x
attributes(x)
dim(x)
dim(x) <- c(3,6)
x
dim(x) <- c(3, 3, 2)
x
?array
x <- c(1,3,2)
x
IrisData <- iris
IrisData$Sepal.Length
order(IrisData$Sepal.Length)
IrisData[order(IrisData$Sepal.Length)]
IrisData[order(IrisData$Sepal.Length)]
IrisData[order(IrisData$Sepal.Length),]
?order
library(rvest)
url = 'http://rice.sinica.edu.tw/TRIM2/showTraits.php'
url=read_html(url)
url=html_nodes(url,"a")
nxturl=html_attr(url,"href")
url = 'http://rice.sinica.edu.tw/TRIM2/showTraits.php'
url=read_html(url)
url=html_nodes(url,"a")
title=html_text(url)
Phenotype <- cbind(title,nxturl)
View(Phenotype)
nxturl
sapply(nxturl,past0("http://rice.sinica.edu.tw/TRIM2/",))
sapply(nxturl,paste0("http://rice.sinica.edu.tw/TRIM2/",))
sapply(nxturl,function(x){
return(paste0("http://rice.sinica.edu.tw/TRIM2/",x))
})
nxturl <- sapply(nxturl,function(x){
return(paste0("http://rice.sinica.edu.tw/TRIM2/",x))
})
Phenotype <- cbind(title,nxturl)
View(Phenotype)
rownames(Phenotype) <- c(1:70)
View(Phenotype)
Phenotype <- Phenotype[c(67,70),]
View(Phenotype)
Phenotype <- cbind(title,nxturl)
rownames(Phenotype) <- c(1:70)
Phenotype <- Phenotype[-c(67,70),]
Phenotype <- cbind(title,nxturl)
rownames(Phenotype) <- c(1:70)
Phenotype <- Phenotype[-c(69,70),]
View(Phenotype)
Phenotype[1]
Phenotype[,1]
Phenotype[1,]
Phenotype[1]
Phenotype[,1]
Phenotype[,2]
Phenotype[1,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[2,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[3,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[4,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant
Phenotype[4,2]
url=read_html(Phenotype[4,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant[1]
FindMutant <- function(x){
url=read_html(Phenotype[x,2])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(2)
write.csv(FindMutant(2),file="test.txt")
FindMutant(2)[1]
write.csv(FindMutant(2),file=FindMutant(2)[1]+".txt")
FindMutant(2)[1]
paste0(FindMutant(2)[1],".txt")
write.csv(FindMutant(2),file=paste0(FindMutant(2)[1],".txt"))
write.csv(FindMutant(2)[2],file=paste0(FindMutant(2)[1],".txt"))
Phenotype[2]
sapply(Phenotype[2], write)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(FindMutant(x)[1],".txt"))
}
sapply(Phenotype[2], write)
Phenotype[2]
Phenotype[,2]
sapply(Phenotype[,2], write)
write(x)
write(2)
sapply(c(1:68), write)
c(1:68)
Phenotype[,2]
sapply(c(1:68), write)
sapply(c(1:68), FindMutant)
sapply(1, FindMutant)
sapply(2, FindMutant)
sapply(50, FindMutant)
sapply(40, FindMutant)
sapply(45, FindMutant)
paste0(FindMutant(2)[1],".txt")
sapply(46, FindMutant)
sapply(48, FindMutant)
sapply(49, FindMutant)
sapply(50, FindMutant)
sapply(c(1:10), write)
sapply(c(11:20), write)
sapply(c(21:30), write)
sapply(c(21:25), write)
sapply(c(21:22), write)
sapply(c(21), write)
sapply(c(22), write)
sapply(c(23), write)
sapply(c(24:30), write)
Phenotype[,2]
findMutant(Phenotype[1,2])
FindMutant(Phenotype[1,2])
Phenotype[1,2]
Phenotype[4,2]
View(Phenotype)
Phenotype[,1]
ct = 1
sapply(Phenotype[,1],function(x){
paste(ct,"_",x)
ct+=1
})
sapply(Phenotype[,1],function(x){
return((ct,"_",x))
ct = ct +1
})
Phenotype <- cbind(c(1:68),Phenotype)
View(Phenotype)
Phenotype[1,]
Phenotype[,1]
Phenotype[1,1]
FindMutant <- function(x){
url=read_html(Phenotype[x,3])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(1)
FindMutant(2)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],FindMutant(x)[1],".txt"))
}
write(1)
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
}
write(1)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],"_",FindMutant(x)[1],".txt"))
}
write(1)
sapply(c(1:68), write)
write(4)
write(5)
write(6)
sapply(c(7:15), write)
sapply(c(16:20), write)
sapply(c(21:25), write)
sapply(c(20:25), write)
sapply(c(22:25), write)
sapply(c(23:25), write)
sapply(c(26:35), write)
sapply(c(36:45), write)
sapply(c(42:45), write)
sapply(c(43:45), write)
sapply(c(46:55), write)
sapply(c(56:65), write)
sapply(c(66:68), write)
data <- as.factor(iris)
iris
data <- as.factor(iris3)
iris3
library(apriori)
install.packages(apriori)
install.packages("apriori")
#載入套件
library(neuralnet)
install.packages("neuralnet")
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
View(data)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
x
y <- logistic(x) + rnorm(100, sd = 0.2)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
print(bpn)
#圖解BP
plot(bpn)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Loading the required packages
require(monmlp)
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
#Clear the workspace
rm(list = ls())
#Loading the required packages
require(monmlp)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
install.packages("kerasR")
iris
data <- iris
View(data)
dataset
datasets::airquality
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
print(bpn)
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
install.packages("nnet")
library("nnet")
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#(4)預測
#test組執行預測
prediction <- predict(nnetM, testdata, type = 'class')
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
data <- iris
View(data)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
cm
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
View(testdata)
View(traindata)
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
gset <- getGEO("GSE19983", GSEMatrix =TRUE, AnnotGPL=FALSE)
install.packages("Biobase")
library(rvest)
library(magrittr)
library(httr)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822")
View(test)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text %>% html_nodes(.,"h1")
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".date") %>% html_text()
print("hello")
shiny::runApp('NTU-CSX-DataScience--Group5/Finalproject/R Shiny/FaceBookAPI')
runApp('NTU-CSX-DataScience--Group5/Finalproject/R Shiny/FaceBookAPI')
runApp('NTU-CSX-DataScience--Group5/Finalproject/R Shiny/FaceBookAPI')
runApp('NTU-CSX-DataScience--Group5/Finalproject/R Shiny/FaceBookAPI')
setwd("~/GitHub/NTU-CSX-DataScience--Group5/Finalproject/R Shiny/FaceBookAPI")
FB_Taipei <- read.csv("FaceBookAPI-Taipei.csv")
colnames(FB_Taipei)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
