apply(data,mean)
apply(data,2,mean)
apply(data,1,mean)
?apply
IrisData <- iris
iris[,5]
library(swirl)
swirl()
1:18
x <- matrix(1:18,6,3)
?matrix
x
attributes(x)
dim(x)
dim(x) <- c(3,6)
x
dim(x) <- c(3, 3, 2)
x
?array
x <- c(1,3,2)
x
IrisData <- iris
IrisData$Sepal.Length
order(IrisData$Sepal.Length)
IrisData[order(IrisData$Sepal.Length)]
IrisData[order(IrisData$Sepal.Length)]
IrisData[order(IrisData$Sepal.Length),]
?order
library(rvest)
url = 'http://rice.sinica.edu.tw/TRIM2/showTraits.php'
url=read_html(url)
url=html_nodes(url,"a")
nxturl=html_attr(url,"href")
url = 'http://rice.sinica.edu.tw/TRIM2/showTraits.php'
url=read_html(url)
url=html_nodes(url,"a")
title=html_text(url)
Phenotype <- cbind(title,nxturl)
View(Phenotype)
nxturl
sapply(nxturl,past0("http://rice.sinica.edu.tw/TRIM2/",))
sapply(nxturl,paste0("http://rice.sinica.edu.tw/TRIM2/",))
sapply(nxturl,function(x){
return(paste0("http://rice.sinica.edu.tw/TRIM2/",x))
})
nxturl <- sapply(nxturl,function(x){
return(paste0("http://rice.sinica.edu.tw/TRIM2/",x))
})
Phenotype <- cbind(title,nxturl)
View(Phenotype)
rownames(Phenotype) <- c(1:70)
View(Phenotype)
Phenotype <- Phenotype[c(67,70),]
View(Phenotype)
Phenotype <- cbind(title,nxturl)
rownames(Phenotype) <- c(1:70)
Phenotype <- Phenotype[-c(67,70),]
Phenotype <- cbind(title,nxturl)
rownames(Phenotype) <- c(1:70)
Phenotype <- Phenotype[-c(69,70),]
View(Phenotype)
Phenotype[1]
Phenotype[,1]
Phenotype[1,]
Phenotype[1]
Phenotype[,1]
Phenotype[,2]
Phenotype[1,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[2,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[3,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[4,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant
Phenotype[4,2]
url=read_html(Phenotype[4,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant[1]
FindMutant <- function(x){
url=read_html(Phenotype[x,2])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(2)
write.csv(FindMutant(2),file="test.txt")
FindMutant(2)[1]
write.csv(FindMutant(2),file=FindMutant(2)[1]+".txt")
FindMutant(2)[1]
paste0(FindMutant(2)[1],".txt")
write.csv(FindMutant(2),file=paste0(FindMutant(2)[1],".txt"))
write.csv(FindMutant(2)[2],file=paste0(FindMutant(2)[1],".txt"))
Phenotype[2]
sapply(Phenotype[2], write)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(FindMutant(x)[1],".txt"))
}
sapply(Phenotype[2], write)
Phenotype[2]
Phenotype[,2]
sapply(Phenotype[,2], write)
write(x)
write(2)
sapply(c(1:68), write)
c(1:68)
Phenotype[,2]
sapply(c(1:68), write)
sapply(c(1:68), FindMutant)
sapply(1, FindMutant)
sapply(2, FindMutant)
sapply(50, FindMutant)
sapply(40, FindMutant)
sapply(45, FindMutant)
paste0(FindMutant(2)[1],".txt")
sapply(46, FindMutant)
sapply(48, FindMutant)
sapply(49, FindMutant)
sapply(50, FindMutant)
sapply(c(1:10), write)
sapply(c(11:20), write)
sapply(c(21:30), write)
sapply(c(21:25), write)
sapply(c(21:22), write)
sapply(c(21), write)
sapply(c(22), write)
sapply(c(23), write)
sapply(c(24:30), write)
Phenotype[,2]
findMutant(Phenotype[1,2])
FindMutant(Phenotype[1,2])
Phenotype[1,2]
Phenotype[4,2]
View(Phenotype)
Phenotype[,1]
ct = 1
sapply(Phenotype[,1],function(x){
paste(ct,"_",x)
ct+=1
})
sapply(Phenotype[,1],function(x){
return((ct,"_",x))
ct = ct +1
})
Phenotype <- cbind(c(1:68),Phenotype)
View(Phenotype)
Phenotype[1,]
Phenotype[,1]
Phenotype[1,1]
FindMutant <- function(x){
url=read_html(Phenotype[x,3])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(1)
FindMutant(2)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],FindMutant(x)[1],".txt"))
}
write(1)
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
}
write(1)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],"_",FindMutant(x)[1],".txt"))
}
write(1)
sapply(c(1:68), write)
write(4)
write(5)
write(6)
sapply(c(7:15), write)
sapply(c(16:20), write)
sapply(c(21:25), write)
sapply(c(20:25), write)
sapply(c(22:25), write)
sapply(c(23:25), write)
sapply(c(26:35), write)
sapply(c(36:45), write)
sapply(c(42:45), write)
sapply(c(43:45), write)
sapply(c(46:55), write)
sapply(c(56:65), write)
sapply(c(66:68), write)
data <- as.factor(iris)
iris
data <- as.factor(iris3)
iris3
library(apriori)
install.packages(apriori)
install.packages("apriori")
#載入套件
library(neuralnet)
install.packages("neuralnet")
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
View(data)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
x
y <- logistic(x) + rnorm(100, sd = 0.2)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
print(bpn)
#圖解BP
plot(bpn)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Loading the required packages
require(monmlp)
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
#Clear the workspace
rm(list = ls())
#Loading the required packages
require(monmlp)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
install.packages("kerasR")
iris
data <- iris
View(data)
dataset
datasets::airquality
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
print(bpn)
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
install.packages("nnet")
library("nnet")
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#(4)預測
#test組執行預測
prediction <- predict(nnetM, testdata, type = 'class')
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
data <- iris
View(data)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
cm
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
View(testdata)
View(traindata)
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
gset <- getGEO("GSE19983", GSEMatrix =TRUE, AnnotGPL=FALSE)
install.packages("Biobase")
library(rvest)
library(magrittr)
library(httr)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822")
View(test)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text %>% html_nodes(.,"h1")
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".date") %>% html_text()
print("hello")
setwd("~/GitHub/NTU-CSX-DataScience--Group5/Finalproject/UDN")
data <- read.csv("Output2")
data1 <- read.csv("Output1")
data2 <- read.csv("Output2")
data3 <- read.csv("Output3")
data1 <- read.csv("Output1")
data2 <- read.csv("Output2")
data3 <- read.csv("Output3")
data1$V1[500]
data2$V1[1]
data2$V1[335]
data3$V1[1]
View(data1)
data <- rbind(data1,data2,data3)
unq <- unique(data)
View(data1)
View(data2)
test <- data[c(2,3,4),]
test <- data[,c(2,3,4)]
View(test)
library(httr)
library(rvest)
library(yaml)
library(magrittr)
news_links = read.csv( file = "Ko_udnlinks")
colnames(news_links) <- c("ID","URL")
news_links$URL <- news_links$URL %>% as.character()
length(news_links$URL)
unique(news_links$URL)
title <- c()
time <- c()
text <- c()
FindNews <- function(URL){
t.doc <-  read_html(URL %>% as.character())
title_node <- html_node(t.doc, "#story_art_title")
t_title <- html_text(title_node)
title <- rbind(title, t_title)
time_node <- html_node(t.doc, "#story_bady_info > div > span")
t_time <- html_text(time_node)
time <- rbind(time, t_time)
text_node <- html_nodes(t.doc, "#story_body_content > p")
t_text <- html_text(text_node)
bindtext <- ""
for(i in c(1:length(t_text))){
bindtext <- paste(bindtext,t_text[i])
}
return(cbind(title, time, bindtext))
}
Output4 <- data.frame()
for( i in 1887:3240){
print(i)
data <- FindNews(news_links[i,2])
Output4 <- rbind(Output4,data)
Sys.sleep(sample(1:10,1))
}
write.csv(Output4,file="Output4")
data1 <- read.csv("Ouput1")
setwd("~/GitHub/NTU-CSX-DataScience--Group5/Finalproject/UDN")
data1 <- read.csv("Ouput1")
data1 <- read.csv("Ouput1")
data1 <- read.csv("Output1")
data2 <- read.csv("Output2")
data3 <- read.csv("Output3")
data4 <- read.csv("Output4")
data1$bindtext[1]
data4 <- read.csv("Output4")
data1$V2[1]
data1$V1[1]
data1$V1[500]
data2$V1[1]
data2$V1[335]
data3$V1[1]
data3$V1[1051]
data4$V1[1]
data4$V1[350]
data <- rbind(data1,data2,data3,data4)
write.csv(data,"ko_Output")
library(httr)
library(rvest)
library(yaml)
library(magrittr)
news_links = read.csv( file = "Ko_udnlinks")
colnames(news_links) <- c("ID","URL")
news_links$URL <- news_links$URL %>% as.character()
#爬每一篇新聞的標題、時間、內文
#這邊要寫成function
length(news_links$URL)
unique(news_links$URL)
title <- c()
time <- c()
text <- c()
FindNews <- function(URL){
t.doc <-  read_html(URL %>% as.character())
title_node <- html_node(t.doc, "#story_art_title")
t_title <- html_text(title_node)
title <- rbind(title, t_title)
time_node <- html_node(t.doc, "#story_bady_info > div > span")
t_time <- html_text(time_node)
time <- rbind(time, t_time)
text_node <- html_nodes(t.doc, "#story_body_content > p")
t_text <- html_text(text_node)
bindtext <- ""
for(i in c(1:length(t_text))){
bindtext <- paste(bindtext,t_text[i])
}
return(cbind(title, time, bindtext))
}
Output5 <- data.frame()
Output5 <- data.frame()
for( i in 2237:3240){
print(i)
data <- FindNews(news_links[i,2])
Output5 <- rbind(Output5,data)
Sys.sleep(sample(1:10,1))
}
#持續少量分次爬
write.csv(Output5,file="Output5")
Output5 <- data.frame()
for( i in 2238:3240){
print(i)
data <- FindNews(news_links[i,2])
Output5 <- rbind(Output5,data)
Sys.sleep(sample(1:10,1))
}
## 2237 掛掉
#持續少量分次爬
write.csv(Output5,file="Output5")
data1 <- read.csv("Output1")
data2 <- read.csv("Output2")
data3 <- read.csv("Output3")
data4 <- read.csv("Output4")
data <- rbind(data1,data2,data3,data4)
2236+1003
write.csv(data,"ko_Output")
write.csv(data,"ko_Output.csv")
write.csv(Output5,"ko_Output2.csv")
s1 <- read.csv("ko_Output.csv")
s2 <- read.csv("ko_Output2.csv")
2236+1003
View(s1)
View(s2)
