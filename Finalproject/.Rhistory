library(help=datasets)
iris <- 1
data(iris,package = "datasets")
head(iris)
colnames(iris)
help(iris)
readBin(lvr_land.path, what = "raw", n = 3)
eadLines(file(lvr_land.path, encoding = "BIG5"),n = 1)
readLines(file(lvr_land.path, encoding = "BIG5"),n = 1)
readLines(file(lvr_land.path, encoding = "BIG5"),n = 5)
file.info(lvr_land.path)
lvr_land.info <- file.info(lvr_land.path)
class(lvr_land.info)
colnames(lvr_land.info)
lvr_land.info$size
lvr_land.bin <- readBin(lvr_land.path,what = "raw", n = 3)
lvr_land.bin <- readBin(lvr_land.path,what = "raw", n = lvr_land.info$size)
library(stringi)
lvr_land.txt <- stri_encode(lvr_land.bin, "BIG-5",
"UTF-8")
read.table(lvr_land.path, fileEncoding = "BIG-5")
lvr_land <- read.table(lvr_land.path,fileEncoding = "BIG5")
lvr_land <- read.table(file(lvr_land.path, encoding = "BIG5"), header =
TRUE, sep = ",")
main()
library(help="datasets")
library(help="datasets")
library(help=datasets)
iris
iris<-1
data(iris,package = "datasets")
head(iris)
colnames(iris)
help(iris)
readBin(lvr_land.path,
what = "raw", n = 3)
data = iris
head(data)
apply(data,2,mean)
apply(data,mean)
head(data)
data[,5]
head(data)
data[,-5]
data = data[,-5]
head(data)
apply(data,mean)
apply(data,2,mean)
apply(data,1,mean)
?apply
IrisData <- iris
iris[,5]
library(swirl)
swirl()
1:18
x <- matrix(1:18,6,3)
?matrix
x
attributes(x)
dim(x)
dim(x) <- c(3,6)
x
dim(x) <- c(3, 3, 2)
x
?array
x <- c(1,3,2)
x
IrisData <- iris
IrisData$Sepal.Length
order(IrisData$Sepal.Length)
IrisData[order(IrisData$Sepal.Length)]
IrisData[order(IrisData$Sepal.Length)]
IrisData[order(IrisData$Sepal.Length),]
?order
library(rvest)
url = 'http://rice.sinica.edu.tw/TRIM2/showTraits.php'
url=read_html(url)
url=html_nodes(url,"a")
nxturl=html_attr(url,"href")
url = 'http://rice.sinica.edu.tw/TRIM2/showTraits.php'
url=read_html(url)
url=html_nodes(url,"a")
title=html_text(url)
Phenotype <- cbind(title,nxturl)
View(Phenotype)
nxturl
sapply(nxturl,past0("http://rice.sinica.edu.tw/TRIM2/",))
sapply(nxturl,paste0("http://rice.sinica.edu.tw/TRIM2/",))
sapply(nxturl,function(x){
return(paste0("http://rice.sinica.edu.tw/TRIM2/",x))
})
nxturl <- sapply(nxturl,function(x){
return(paste0("http://rice.sinica.edu.tw/TRIM2/",x))
})
Phenotype <- cbind(title,nxturl)
View(Phenotype)
rownames(Phenotype) <- c(1:70)
View(Phenotype)
Phenotype <- Phenotype[c(67,70),]
View(Phenotype)
Phenotype <- cbind(title,nxturl)
rownames(Phenotype) <- c(1:70)
Phenotype <- Phenotype[-c(67,70),]
Phenotype <- cbind(title,nxturl)
rownames(Phenotype) <- c(1:70)
Phenotype <- Phenotype[-c(69,70),]
View(Phenotype)
Phenotype[1]
Phenotype[,1]
Phenotype[1,]
Phenotype[1]
Phenotype[,1]
Phenotype[,2]
Phenotype[1,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[2,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[3,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
Phenotype[4,2]
url=read_html(Phenotype[1,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant
Phenotype[4,2]
url=read_html(Phenotype[4,2])
url=html_nodes(url,"td")
mutant=html_text(url)
mutant[2]
mutant[1]
FindMutant <- function(x){
url=read_html(Phenotype[x,2])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(2)
write.csv(FindMutant(2),file="test.txt")
FindMutant(2)[1]
write.csv(FindMutant(2),file=FindMutant(2)[1]+".txt")
FindMutant(2)[1]
paste0(FindMutant(2)[1],".txt")
write.csv(FindMutant(2),file=paste0(FindMutant(2)[1],".txt"))
write.csv(FindMutant(2)[2],file=paste0(FindMutant(2)[1],".txt"))
Phenotype[2]
sapply(Phenotype[2], write)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(FindMutant(x)[1],".txt"))
}
sapply(Phenotype[2], write)
Phenotype[2]
Phenotype[,2]
sapply(Phenotype[,2], write)
write(x)
write(2)
sapply(c(1:68), write)
c(1:68)
Phenotype[,2]
sapply(c(1:68), write)
sapply(c(1:68), FindMutant)
sapply(1, FindMutant)
sapply(2, FindMutant)
sapply(50, FindMutant)
sapply(40, FindMutant)
sapply(45, FindMutant)
paste0(FindMutant(2)[1],".txt")
sapply(46, FindMutant)
sapply(48, FindMutant)
sapply(49, FindMutant)
sapply(50, FindMutant)
sapply(c(1:10), write)
sapply(c(11:20), write)
sapply(c(21:30), write)
sapply(c(21:25), write)
sapply(c(21:22), write)
sapply(c(21), write)
sapply(c(22), write)
sapply(c(23), write)
sapply(c(24:30), write)
Phenotype[,2]
findMutant(Phenotype[1,2])
FindMutant(Phenotype[1,2])
Phenotype[1,2]
Phenotype[4,2]
View(Phenotype)
Phenotype[,1]
ct = 1
sapply(Phenotype[,1],function(x){
paste(ct,"_",x)
ct+=1
})
sapply(Phenotype[,1],function(x){
return((ct,"_",x))
ct = ct +1
})
Phenotype <- cbind(c(1:68),Phenotype)
View(Phenotype)
Phenotype[1,]
Phenotype[,1]
Phenotype[1,1]
FindMutant <- function(x){
url=read_html(Phenotype[x,3])
url=html_nodes(url,"td")
mutant=html_text(url)
return(c(mutant[1],mutant[2]))
}
FindMutant(1)
FindMutant(2)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],FindMutant(x)[1],".txt"))
}
write(1)
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype,"_",FindMutant(x)[1],".txt"))
}
write(1)
write <- function(x){
write.csv(FindMutant(x)[2],file=paste0(Phenotype[x,1],"_",FindMutant(x)[1],".txt"))
}
write(1)
sapply(c(1:68), write)
write(4)
write(5)
write(6)
sapply(c(7:15), write)
sapply(c(16:20), write)
sapply(c(21:25), write)
sapply(c(20:25), write)
sapply(c(22:25), write)
sapply(c(23:25), write)
sapply(c(26:35), write)
sapply(c(36:45), write)
sapply(c(42:45), write)
sapply(c(43:45), write)
sapply(c(46:55), write)
sapply(c(56:65), write)
sapply(c(66:68), write)
data <- as.factor(iris)
iris
data <- as.factor(iris3)
iris3
library(apriori)
install.packages(apriori)
install.packages("apriori")
#載入套件
library(neuralnet)
install.packages("neuralnet")
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
View(data)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
x
y <- logistic(x) + rnorm(100, sd = 0.2)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
print(bpn)
#圖解BP
plot(bpn)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Loading the required packages
require(monmlp)
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
mlpModel <- monmlp.predict(x = x, weights = mlpModel)
#Plotting predicted value over actual values
for(i in 1:15){
lines(x, attr(mlpModel, "ensemble")[[i]], col = "red")
}
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
cat ("MSE for Gradient Descent Trained Model: ", mse(y, mlpModel))
#Clear the workspace
rm(list = ls())
#Loading the required packages
require(monmlp)
require(Metrics)
#MultiLayer Perceptron Code
x <- as.matrix(seq(-10, 10, length = 100))
y <- logistic(x) + rnorm(100, sd = 0.2)
#Plotting Data
plot(x, y)
lines(x, logistic(x), lwd = 10, col = "gray")
#Fitting Model
mlpModel <- monmlp.fit(x = x, y = y, hidden1 = 3, monotone = 1,
n.ensemble = 15, bag = TRUE)
install.packages("kerasR")
iris
data <- iris
View(data)
dataset
datasets::airquality
#載入套件
library(neuralnet)
#整理資料
data <- iris
data$setosa <- ifelse(data$Species == "setosa", 1, 0)
data$versicolor <- ifelse(data$Species == "versicolor", 1, 0)
data$virginica <- ifelse(data$Species == "virginica", 1, 0)
#訓練模型
f1 <- as.formula('setosa + versicolor + virginica  ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width')
print(bpn)
bpn <- neuralnet(formula = f1, data = data, hidden = c(2,4),learningrate = 0.01)
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#
source_url('https://gist.githubusercontent.com/Peque/41a9e20d6687f2f3108d/raw/85e14f3a292e126f1454864427e3a189c2fe33f3/nnet_plot_update.r')
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
#範例使用irisdata
data(iris)
#(2)分為訓練組和測試組資料集
set.seed(1117)
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
#訓練組樣本
traindata <- iris[t_idx,]
#測試組樣本
testdata <- iris[ - t_idx,]
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
install.packages("nnet")
library("nnet")
nnetM <- nnet(formula = Species ~ ., linout = T, size = 3, decay = 0.001, maxit = 1000, trace = T, data = traindata)
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#(4)預測
#test組執行預測
prediction <- predict(nnetM, testdata, type = 'class')
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
data <- iris
View(data)
library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
#(3)畫圖
plot.nnet(nnetM, wts.only = F)
#預測結果
cm <- table(x = testdata$Species, y = prediction, dnn = c("實際", "預測"))
cm
#取得總筆數
n <- nrow(iris)
#設定訓練樣本數70%
t_size = round(0.7 * n)
#取出樣本數的idx
t_idx <- sample(seq_len(n), size = t_size)
View(testdata)
View(traindata)
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
################################################################
#   Differential expression analysis with limma
library(Biobase)
install.packages("GEOquery")
gset <- getGEO("GSE19983", GSEMatrix =TRUE, AnnotGPL=FALSE)
install.packages("Biobase")
library(rvest)
library(magrittr)
library(httr)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822")
View(test)
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_text %>% html_nodes(.,"h1")
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".data") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,"h1") %>% html_text()
test <- read_html("http://ent.ltn.com.tw/news/breakingnews/2443822") %>% html_nodes(.,".date") %>% html_text()
print("hello")
setwd("~/GitHub/NTU-CSX-DataScience--Group5/Finalproject")
library(stringr)
library(jiebaR)
pos<-read.csv('NTUSD_positive_UTF-8.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(1, length(pos[,1])) #正面情感词语赋权重为1
pos <- cbind(pos, weight)
neg<-read.csv('NTUSD_negative_UTF-8.csv',header=F,stringsAsFactors=FALSE,encoding = "unicode")
weight <- rep(-1, length(neg[,1])) #负面情感词语赋权重为-1
neg <- cbind(neg, weight)
posneg<-rbind(pos,neg)
colnames(posneg)<-c('term','weight')
user<-posneg[,'term']
Data <- read.csv("Di_JanFebNews.csv")
text <- Data$V3[1]
text
w1<-worker()
new_user_word(w1,user)
# 文字清理
library(NLP)
library(tm)
library(jiebaRD)
library(jiebaR)
docs <- Corpus(VectorSource(Data$V3))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
docs <- tm_map(docs,toSpace,"\n")
docs <- tm_map(docs,toSpace, "[A-Za-z0-9]")
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, toSpace, "、")
docs <- tm_map(docs, toSpace, "，")
docs <- tm_map(docs, toSpace, "。")
docs <- tm_map(docs, toSpace, "！")
docs <- tm_map(docs, toSpace, "「")
docs <- tm_map(docs, toSpace, "（")
docs <- tm_map(docs, toSpace, "」")
docs <- tm_map(docs, toSpace, "）")
docs <- tm_map(docs, toSpace, "\n")
docs <- tm_map(docs, toSpace, "；")
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, toSpace, "[a-zA-Z]")
docs <- tm_map(docs, stripWhitespace)
text <- docs[[1]]$content
text<-list(text)
res<-segment(unlist(text),w1)
res
temp<-data.frame()
temp[c(1:length(res)),1]<-rep('1.text' ,length(res)) #id
temp[c(1:length(res)),2]<-res[1:length(res)]#term
colnames(temp)<-c('id','term')
library(plyr)
temp<-join(temp,posneg,by='term')
temp<-temp[!is.na(temp$weight),]
head(temp)
#计算情感指数
senti<-sum(temp$weight)
senti
Ct_pos <- temp[temp$weight==1,3] %>% length()
Ct_neg <- temp[temp$weight==-1,3] %>% length()
Ct_pos/(Ct_pos+Ct_neg)
res
# word freq
library(tidyr)
library(dplyr)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(readtext)
library(jiebaRD)
library(jiebaR)
library(slam)
library(Matrix)
library(tidytext)
mixseg = worker()
jieba_tokenizer = function(d){
unlist(segment(d[[1]], mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
row.names(DF) <- DF$.rownames
# Take a look at a subset of DF
DF <- DF %>% t
head(DF,10)
Data <- DF[,2:10]
Data <- Data %>% t
DF <- tidy(tf)
row.names(DF) <- DF$.rownames
Data <- DF[,2:10]
