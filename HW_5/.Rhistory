rule <- apriori(iris,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(default = "lhs",
rhs = c("setosa", "versicolor", "virginica")))
rule <- apriori(data,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(default = "lhs",
rhs = c("setosa", "versicolor", "virginica")))
rule <- apriori(data,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(default = "lhs",
rhs = c("Species = setosa", "Species = versicolor", "Species = virginica")))
data$Species
?apriori
rule <- apriori(data,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(items = c("Species = setosa", "Species = versicolor", "Species = virginica")))
rule <- apriori(data,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(items = c("Species=setosa", "Species=versicolor", "Species=virginica")))
inspect(rule)
inspect(rule)
inspect(rule)
rule <- apriori(data,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(default = "lhs",
rhs = c("Species=setosa", "Species=versicolor", "Species=virginica")))
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph", control=list(type="items"))
plot(sort.rule, method="grouped")
rm(list=ls())
library(magrittr)
datasets::iris
str(iris)
data <- iris
data$Species <- data$Species %>% as.factor()
summary(data)
Class<- function(Ary){
DATA = c()
Min = min(Ary)
Max = max(Ary)
Inter = (Max - Min)/3
Output <- sapply(Ary,function(x){
if(Min<= x && x < Min+Inter){
DATA=c(DATA,"small")
}else if(Min+Inter<= x && x < Min+Inter*2){
DATA=c(DATA,"median")
}else if(Min+Inter*2<= x && x <= Min+Inter*3){
DATA=c(DATA,"large")
}
})
return(Output)
}
data$Sepal.Length <- Class(data$Sepal.Length) %>% as.factor()
data$Sepal.Width <- Class(data$Sepal.Width) %>% as.factor()
data$Petal.Length <- Class(data$Petal.Length) %>% as.factor()
data$Petal.Width <- Class(data$Petal.Width) %>% as.factor()
require(arules)
rule <- apriori(data,
parameter = list(minlen=4, supp=0.1, conf=0.7),
appearance = list(default = "lhs",
rhs = c("Species=setosa", "Species=versicolor", "Species=virginica")))
inspect(rule)
sort.rule <- sort(rule, by="lift")
inspect(sort.rule)
require(arulesViz)
plot(sort.rule)
plot(sort.rule, method="graph", control=list(type="items"))
plot(sort.rule, method="grouped")
install.packages("kerasR")
library(kerasR)
install.packages("keras")
install.packages("tensorflow")
library(tensorflow)
library(keras)
library(keras)
library(tensorflow)
library(keras)
install.packages("tensorflow")
install.packages("keras")
library(keras)
library(tensorflow)
# Test Keras: Define a simple DNN network
require(keras)
model <- keras_model_sequential()
model %>%
layer_dense(units = 256, activation = "relu", input_shape = c(784)) %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 128, activation = "relu") %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 10, activation = "softmax")
summary(model)
data <- dataset_mnist()
data
View(data)
library(keras)
library(tensorflow)
data <- dataset_mnist()
train_x<-data$train$x
train_y<-data$train$y
test_x<-data$test$x
test_y<-data$test$y
rm(data)
train_x <- array(train_x, dim = c(dim(train_x)[1], prod(dim(train_x)[-1]))) / 255
test_x <- array(test_x, dim = c(dim(test_x)[1], prod(dim(test_x)[-1]))) / 255
train_y<-to_categorical(train_y,10)
test_y<-to_categorical(test_y,10)
model <- keras_model_sequential()
model %>%
layer_dense(units = 784, input_shape = 784) %>%
layer_dropout(rate=0.4)%>%
layer_activation(activation = 'relu') %>%
layer_dense(units = 10) %>%
layer_activation(activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = 'adam',
metrics = c('accuracy')
)
model %>% fit(train_x, train_y, epochs = 100, batch_size = 128)
loss_and_metrics <- model %>% evaluate(test_x, test_y, batch_size = 128)
loss_and_metrics
library(rvest)
library(magrittr)
library(httr)
library(dplyr)
library(rvest)
library(magrittr)
library(httr)
library(dplyr)
data =read_html("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2") %>% html_text(trim = T)
prefix <- "https://www.ptt.cc/bbs/Gossiping/search?page="
FindURL <- function(URL){
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
URLlink <- session_redirected %>%
html_nodes(".title a") %>% html_attr(.,"href")
title <- session_redirected %>%
html_nodes(".title a") %>% html_text
Date <- session_redirected %>%
html_nodes(".date") %>% html_text
output <- cbind(title,URLlink,Date)
return(output)
}
drko <- data.frame()
for(i in c(1:238))
{
x <- paste0(prefix, i , "&q=%E6%9F%AF%E6%96%87%E5%93%B2")
drko <- rbind(drko, FindURL(x))
}
url <- drko$URLlink
?htmlparse
?htmlParse
library(RCurl)
library(bitops)
library(RCurl)
library(XML)
?htmlParse
url <- drko$URLlink
res_drko <- sapply(drko$title,function(x){gsub("\\s", replacement = "_",x)})
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(url)
{
html <- htmlParse( getURL(url) )
session = rvest::html_session(url = url)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
doc
sapply(url, getdoc)
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(x)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = x)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
sapply(url, getdoc)
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
sapply(x, getdoc)
library(rvest)
library(magrittr)
library(httr)
library(dplyr)
data =read_html("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2") %>% html_text(trim = T)
prefix <- "https://www.ptt.cc/bbs/Gossiping/search?page="
FindURL <- function(URL){
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
URLlink <- session_redirected %>%
html_nodes(".title a") %>% html_attr(.,"href")
title <- session_redirected %>%
html_nodes(".title a") %>% html_text
Date <- session_redirected %>%
html_nodes(".date") %>% html_text
output <- cbind(title,URLlink,Date)
return(output)
}
drko <- data.frame()
for(i in c(1:10))
{
x <- paste0(prefix, i , "&q=%E6%9F%AF%E6%96%87%E5%93%B2")
drko2 <- rbind(drko, FindURL(x))
}
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
ssion_redirected = rvest::submit_form(session = session, form = form )
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
temp <- gsub( "  ", " 0", unlist(time) )
part <- strsplit( temp, split=" ", fixed=T )
timestamp <- part[[1]][4]
timestamp <- strsplit( timestamp, split=":", fixed=T )
hour <- timestamp[[1]][1]
print(hour)
}
sapply(x, getdoc)
# result <- FindURL("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2")
# View(result)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
library(rvest)
library(magrittr)
library(httr)
library(dplyr)
data =read_html("https://www.ptt.cc/bbs/Gossiping/search?page=1&q=%E6%9F%AF%E6%96%87%E5%93%B2") %>% html_text(trim = T)
prefix <- "https://www.ptt.cc/bbs/Gossiping/search?page="
FindURL <- function(URL){
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
URLlink <- session_redirected %>%
html_nodes(".title a") %>% html_attr(.,"href")
title <- session_redirected %>%
html_nodes(".title a") %>% html_text
Date <- session_redirected %>%
html_nodes(".date") %>% html_text
output <- cbind(title,URLlink,Date)
return(output)
}
drko <- data.frame()
for(i in c(1:10))
{
x <- paste0(prefix, i , "&q=%E6%9F%AF%E6%96%87%E5%93%B2")
drko2 <- rbind(drko, FindURL(x))
}
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
time <- session_redirected %>%
html_nodes(".article-metaline+ .article-metaline .article-meta-value") %>% html_text
}
sapply(x, getdoc)
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
sapply(x, getdoc)
getdoc <- function(URL)
{
# html <- htmlParse(getURL(x))
session = rvest::html_session(url = URL)
form = session %>%
html_node("form") %>%
html_form()
session_redirected = rvest::submit_form(session = session, form = form )
doc <- session_redirected %>%
html_nodes("#main-content") %>% html_text
}
sapply(x, getdoc)
shiny::runApp('GitHub/NTU-CSX-DataScience--Group5/Finalproject/R Shiny/News_Shiny')
# ===== TFIDF程式 : 臉書
# ===== 處理柯文哲1月至5月的文章
# ------------------------------------------
#
# 匯入套件
library(tibble)
library(dplyr)
library(NLP)
library(tm)
library(stats)
library(proxy)
library(jiebaRD)
library(jiebaR)
library(magrittr)
library(slam)
library(Matrix)
library(tidytext)
library(tidyr)
library(ggplot2)
library(RColorBrewer)
library(wordcloud)
# ===== 資料清理
# 匯入資料組
setwd("/Users/Weber/Documents/GitHub/Weber1234/HW_5")
data <- read.csv("Ko_report.csv")
data <- data %>% na.omit()
# 切開時間
data <- data %>% separate(time, c("year","month","day"),"-")
data <- data %>% separate(day, c("date","time"), "T")
# 排序
data <- data[with(data, order(year ,month, date)), ]
# 清理重複資料
data <- data[!duplicated(data$post), ]
# 移除2017資料
data <- data[data$year == "2018",]
row.names(data) = c(1:138) # 由資料數重新編排號碼
# 依月份建立子資料組
# ===== 切詞
# ---- Jan
docs <- Corpus(VectorSource(data$post))
toSpace <- content_transformer(function(x,pattern){
return(gsub(pattern," ",x))
})
# 刪去單詞贅字、英文字母、標點符號、數字與空格
docs <- tm_map(docs,toSpace,"\n")
clean_doc <- function(docs){
clean_words <- c("[A-Za-z0-9]","、","《","『","』","【","】","／","，","。","！","「","（","」","）","\n","；",">","<","＜","＞")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_doc(docs)
clean_word_doc <- function(docs){
clean_words <- c("我們","他們","的","也","都","就","與","但","是","在","和","及","為","或","且","有","含")
for(i in 1:length(clean_words)){
docs <- tm_map(docs,toSpace, clean_words[i])
}
return(docs)
}
docs <- clean_word_doc(docs)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, removePunctuation)
# 匯入自定義字典
mixseg = worker()
segment <- c("柯文哲","姚文智","丁守中","台北市長","選舉","候選人","台灣","選票","柯市長","民進黨","國民黨","台北市民","市民")
new_user_word(mixseg,segment)
# 有詞頻之後就可以去畫文字雲
jieba_tokenizer=function(d){
unlist(segment(d[[1]],mixseg))
}
seg = lapply(docs, jieba_tokenizer)
freqFrame = as.data.frame(table(unlist(seg)))
# 清除單字
for(i in c(1:length(freqFrame$Var1))){
if((freqFrame$Var1[i] %>% as.character %>% nchar) == 1){
freqFrame[i,] <- NA
}
}
freqFrame <- na.omit(freqFrame)
d.corpus <- Corpus(VectorSource(seg))
tdm <- TermDocumentMatrix(d.corpus)
tf <- as.matrix(tdm)
DF <- tidy(tf)
head(DF, 10)
# ===== 建立TF-IDF
N = tdm$ncol
tf <- apply(tdm, 2, sum)
idfCal <- function(word_doc)
{
log2( N / nnzero(word_doc) )
}
idf <- apply(tdm, 1, idfCal)
doc.tfidf <- as.matrix(tdm)
for(x in 1:nrow(tdm))
{
for(y in 1:ncol(tdm))
{
doc.tfidf[x,y] <- (doc.tfidf[x,y] / tf[y]) * idf[x]
}
}
findZeroId = as.matrix(apply(doc.tfidf, 1, sum))
# tfidfnn = doc.tfidf[-which(findZeroId == 0),]
head(doc.tfidf, 10)
# 畫個助畫柱狀圖
freq=rowSums(as.matrix(doc.tfidf))
high.freq=tail(sort(freq),n=20)
high.freq
hfp.df=as.data.frame(sort(high.freq))
names(hfp.df) <- "frequence"
hfp.df$names <- rownames(hfp.df)
hfp.df <- hfp.df[-c(14,16,17,19,20),]
hfp.df <- hfp.df[order(hfp.df$frequence),]
rownames(hfp.df) <- c(1:15)
ggplot(hfp.df, aes(hfp.df$names, hfp.df$frequence)) +
geom_bar(stat="identity", fill="olivedrab", colour="black") +
xlab("Frequency") + ylab("Terms") +
ggtitle("Term frequencies")
# 順便畫一個文字雲來看看結果
wordcloud(freqFrame$Var1,freqFrame$Freq,
min.freq=30,
random.order=TRUE,random.color=TRUE,
rot.per=.1, colors=rainbow(length(row.names(freqFrame))),
ordered.colors=FALSE,use.r.layout=FALSE,
fixed.asp=TRUE)
